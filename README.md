# ğŸ“˜ NumPy Convolutional Neural Network (CNN) From Scratch

A fully handcrafted **Convolutional Neural Network** for MNIST digit recognition, built using **only NumPy and Pythonâ€™s math module** â€”  
**no PyTorch, no TensorFlow, no external ML libraries.**

This project implements:

- âœ”ï¸ Custom convolution layers  
- âœ”ï¸ Custom ReLU  
- âœ”ï¸ Custom max-pooling  
- âœ”ï¸ Custom flattening  
- âœ”ï¸ Custom fully connected layers  
- âœ”ï¸ Custom forward propagation  
- âœ”ï¸ Custom cross-entropy loss (logits version)  
- âœ”ï¸ Numerical gradient descent (finite difference method)  
- âœ”ï¸ 100% manual implementation of everything  

This project is intended as a **mathematical + educational reproduction of deep learning mechanics**, not a training-optimized CNN.

---

## ğŸŒŸ Motivation

The goal of this project is to understand **every internal detail** of a CNN:

- How convolution works at the pixel level  
- How pooling compresses features  
- How logits become class scores  
- How loss is computed mathematically  
- How weights are updated using numerical gradients  

This is how neural networks were first implemented in academic research before modern frameworks existed.

---

## ğŸš€ Features

### ğŸ”§ 1. Convolution Layer (from scratch)
- Sliding windows over images  
- Multi-channel convolutions  
- Learnable filters  
- Manual patch extraction  
- Fully nested loops (no shortcuts)

### âš¡ 2. Max-Pooling (from scratch)
- 2Ã—2 pooling  
- Fully manual selection of max values  
- No library tricks  

### ğŸ§  3. Forward Propagation  
The full forward pass is computed manually:
Image â†’ Conv1 â†’ Conv2 â†’ MaxPool â†’ Flatten â†’ FC â†’ Logits

Gradients computed using:

\[
\frac{L(W + h) - L(W)}{h}
\]

This is the mathematically pure way to approximate derivatives.

### ğŸ”’ 5. Zero external ML libraries  
Only:

- `numpy`  
- `math`  
- `torchvision` (for data loading only, not ML)

---
## ğŸ“Š Dataset

This project uses **MNIST Handwritten Digits**:

- **60,000 training images**  
- **10,000 testing images**  
- Grayscale (1 channel)  
- Resolution: 28Ã—28  

Images are normalized and zero-padded to 30Ã—30.

---

## ğŸ§® Mathematical Correctness

This project implements:

- âœ”ï¸ Logits  
- âœ”ï¸ Softmax (implicitly inside cross-entropy)  
- âœ”ï¸ Cross-entropy loss  
- âœ”ï¸ Numerical gradient estimation  
- âœ”ï¸ Weight update rule:

\[
W = W - \eta \cdot \frac{\partial L}{\partial W}
\]

Everything is **mathematically valid, exact, and correct**.

This behaves exactly as expected for a numerical gradient checker.

---

## âš ï¸ Performance Notice

This implementation uses **finite differences** for gradient estimation.

This means:

- Extremely slow  
- Not meant for real training  
- Intended for learning and understanding  

This is **NOT** a high-performance CNN â€”  
This is an **educational deep-learning engine** built from scratch.

---

## ğŸ† Why This Project is Special

- No copying  
- No shortcuts  
- No high-level libraries  
- 100% conceptual understanding  
- Everything written manually  
- Demonstrates true ML intuition

---

## ğŸ“Œ Future Work

You can extend this project by adding:

- Backpropagation (analytic gradients)  
- Batch training  
- Better activation functions  
- Regularization  
- Momentum or Adam  
- More conv layers  
- Visualization of filters  

---

## ğŸ™Œ Author

Built entirely from scratch by **Sri Hari S**,  
using only:

- NumPy  
- math  
- deep understanding  
- curiosity  
- and pure logic.
